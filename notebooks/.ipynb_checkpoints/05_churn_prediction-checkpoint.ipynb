{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "277557f2",
   "metadata": {},
   "source": [
    "# 1. Setup & Imports\n",
    "Initial environment setup: imports all necessary libraries for data analysis, machine learning, and visualization. Defines styling configurations for graphs and suppresses warnings for cleaner output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b988e7cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Setup & Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from google.cloud import bigquery\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, classification_report, confusion_matrix,\n",
    "    roc_curve, precision_recall_curve, accuracy_score\n",
    ")\n",
    "import xgboost as xgb\n",
    "\n",
    "# Warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style for visualizations\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"All libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb1fe1e",
   "metadata": {},
   "source": [
    "# 2. Data Loading (Query SQL)\n",
    "Queries data from BigQuery, unifying information from multiple tables (mart_customer_ltv, mart_customer_rfm, mart_cohort_retention, etc.) into a single dataset. Defines the target variable is_churned based on the 180-day no-purchase rule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c25459",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Data Loading (Query SQL)\n",
    "client = bigquery.Client()\n",
    "\n",
    "PROJECT_ID = \"quintoandar-ecommerce-analysis\"\n",
    "\n",
    "query = f\"\"\"\n",
    "WITH customer_data AS (\n",
    "    SELECT \n",
    "        ltv.customer_id,\n",
    "        ltv.customer_state,\n",
    "        ltv.total_orders,\n",
    "        ltv.total_revenue,\n",
    "        ltv.avg_order_value,\n",
    "        ltv.customer_lifespan_days,\n",
    "        ltv.first_purchase_date,\n",
    "        ltv.last_purchase_date,\n",
    "        \n",
    "        rfm.recency_days,\n",
    "        rfm.recency_score,\n",
    "        rfm.frequency_score,\n",
    "        rfm.monetary_score,\n",
    "        rfm.rfm_segment,\n",
    "        \n",
    "        -- Calculate days since first purchase\n",
    "        DATE_DIFF(CURRENT_DATE(), DATE(ltv.first_purchase_date), DAY) as days_since_first_purchase,\n",
    "        \n",
    "        -- Calculate months active\n",
    "        DATE_DIFF(DATE(ltv.last_purchase_date), DATE(ltv.first_purchase_date), MONTH) + 1 as months_active\n",
    "        \n",
    "    FROM `{PROJECT_ID}.olist_marts.mart_customer_ltv` ltv\n",
    "    LEFT JOIN `{PROJECT_ID}.olist_marts.mart_customer_rfm` rfm \n",
    "        ON ltv.customer_id = rfm.customer_id\n",
    "    WHERE ltv.customer_id IS NOT NULL\n",
    "    AND ltv.last_purchase_date IS NOT NULL\n",
    "    AND ltv.first_purchase_date IS NOT NULL\n",
    ")\n",
    "\n",
    "SELECT \n",
    "    *,\n",
    "    -- Define churn (180 days without purchase)\n",
    "    CASE \n",
    "        WHEN DATE_DIFF(CURRENT_DATE(), DATE(last_purchase_date), DAY) > 180 THEN 1\n",
    "        ELSE 0\n",
    "    END as is_churned\n",
    "FROM customer_data\n",
    "\"\"\"\n",
    "\n",
    "print(\"Executing BigQuery...\")\n",
    "try:\n",
    "    df = client.query(query).to_dataframe()\n",
    "    print(f\"Dataset loaded successfully with {len(df)} rows and {len(df.columns)} columns\")\n",
    "    print(f\"Churn rate: {df['is_churned'].mean():.2%}\")\n",
    "    print(\"Available columns:\")\n",
    "    print(df.columns.tolist())\n",
    "except Exception as e:\n",
    "    print(f\"Error executing query: {e}\")\n",
    "    print(\"Creating sample data for demonstration...\")\n",
    "    \n",
    "    np.random.seed(42)\n",
    "    n_samples = 1000\n",
    "    df = pd.DataFrame({\n",
    "        'customer_id': [f'CUST_{i:04d}' for i in range(n_samples)],\n",
    "        'customer_state': np.random.choice(['SP', 'RJ', 'MG', 'RS', 'PR', 'SC', 'BA'], n_samples),\n",
    "        'total_orders': np.random.randint(1, 50, n_samples),\n",
    "        'total_revenue': np.random.exponential(1000, n_samples).round(2),\n",
    "        'avg_order_value': np.random.uniform(50, 500, n_samples).round(2),\n",
    "        'customer_lifespan_days': np.random.randint(30, 365*2, n_samples),\n",
    "        'recency_days': np.random.randint(1, 365, n_samples),\n",
    "        'recency_score': np.random.randint(1, 6, n_samples),\n",
    "        'frequency_score': np.random.randint(1, 6, n_samples),\n",
    "        'monetary_score': np.random.randint(1, 6, n_samples),\n",
    "        'rfm_segment': np.random.choice(['Champions', 'Loyal Customers', 'Potential Loyalists', \n",
    "                                         'Recent Customers', 'Promising', 'Needs Attention', \n",
    "                                         'About to Sleep', 'At Risk', 'Can''t Lose Them', 'Hibernating'], n_samples),\n",
    "        'days_since_first_purchase': np.random.randint(60, 730, n_samples),\n",
    "        'months_active': np.random.randint(1, 24, n_samples),\n",
    "        'is_churned': np.random.binomial(1, 0.3, n_samples)\n",
    "    })\n",
    "    print(f\"Sample dataset created with {len(df)} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f46fc2d",
   "metadata": {},
   "source": [
    "# 3. EDA - Target Variable\n",
    "Initial exploratory analysis focused on the target variable: visualizes churn distribution (pie chart and bar plot), checks for class imbalance, and provides basic statistics on churn rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b3331a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. EDA - Target Variable\n",
    "if len(df) > 0:\n",
    "    # Target distribution\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Pie chart\n",
    "    churn_counts = df['is_churned'].value_counts()\n",
    "    axes[0].pie(churn_counts.values, labels=['Not Churned', 'Churned'], \n",
    "                autopct='%1.1f%%', colors=['lightblue', 'salmon'])\n",
    "    axes[0].set_title('Churn Distribution')\n",
    "    \n",
    "    # Bar plot\n",
    "    sns.barplot(x=churn_counts.index, y=churn_counts.values, ax=axes[1])\n",
    "    axes[1].set_title('Churn Count')\n",
    "    axes[1].set_xlabel('Churn Status')\n",
    "    axes[1].set_ylabel('Count')\n",
    "    axes[1].set_xticks([0, 1])\n",
    "    axes[1].set_xticklabels(['Not Churned (0)', 'Churned (1)'])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Check for class imbalance\n",
    "    churn_rate = df['is_churned'].mean()\n",
    "    if churn_rate < 0.2 or churn_rate > 0.8:\n",
    "        print(f\"Warning: Significant class imbalance detected. Churn rate: {churn_rate:.2%}\")\n",
    "    else:\n",
    "        print(f\"Churn rate is within acceptable range: {churn_rate:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1123efbe",
   "metadata": {},
   "source": [
    "# 4. Feature Engineering\n",
    "Creation of derived features from original variables: engagement_score, logarithmic transformations, purchase frequency per month, average order value trend, purchase acceleration, review sentiment, delivery issues, and geographic region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c9e81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Feature Engineering\n",
    "# Create derived features\n",
    "if len(df) > 0:\n",
    "    # A) Create derived features from available columns\n",
    "    # Engagement Score\n",
    "    if all(col in df.columns for col in ['frequency_score', 'monetary_score', 'recency_score']):\n",
    "        df['engagement_score'] = (\n",
    "            df['frequency_score'] * 0.4 + \n",
    "            df['monetary_score'] * 0.3 + \n",
    "            df['recency_score'] * 0.3\n",
    "        )\n",
    "    else:\n",
    "        df['engagement_score'] = 3.0  # Default value if scores missing\n",
    "    \n",
    "    # Time transformations\n",
    "    if 'recency_days' in df.columns:\n",
    "        df['recency_log'] = np.log1p(df['recency_days'])\n",
    "    \n",
    "    # Purchase frequency per month\n",
    "    if all(col in df.columns for col in ['total_orders', 'months_active']):\n",
    "        df['orders_per_month'] = df['total_orders'] / np.maximum(df['months_active'], 1)\n",
    "    \n",
    "    # AOV trend (compared to overall average)\n",
    "    if 'avg_order_value' in df.columns:\n",
    "        overall_avg = df['avg_order_value'].mean()\n",
    "        df['aov_trend'] = df['avg_order_value'] / overall_avg if overall_avg > 0 else 1\n",
    "    \n",
    "    # Purchase acceleration (simplified)\n",
    "    if 'customer_lifespan_days' in df.columns and 'total_orders' in df.columns:\n",
    "        # Orders per day\n",
    "        df['purchase_acceleration'] = df['total_orders'] / np.maximum(df['customer_lifespan_days'], 1)\n",
    "    \n",
    "    # Geographic region\n",
    "    region_mapping = {\n",
    "        'SP': 'SE', 'RJ': 'SE', 'MG': 'SE', 'ES': 'SE',\n",
    "        'RS': 'S', 'SC': 'S', 'PR': 'S',\n",
    "        'BA': 'NE', 'PE': 'NE', 'CE': 'NE', 'MA': 'NE', 'PB': 'NE', 'RN': 'NE',\n",
    "        'AM': 'N', 'PA': 'N', 'AC': 'N', 'RO': 'N', 'RR': 'N', 'AP': 'N',\n",
    "        'GO': 'CO', 'MT': 'CO', 'MS': 'CO', 'DF': 'CO'\n",
    "    }\n",
    "    \n",
    "    if 'customer_state' in df.columns:\n",
    "        df['customer_region'] = df['customer_state'].map(region_mapping).fillna('Other')\n",
    "    \n",
    "    print(f\"Created {len([col for col in df.columns if col not in ['customer_id', 'is_churned']])} total features\")\n",
    "    \n",
    "    # Show new features\n",
    "    new_features = ['engagement_score', 'recency_log', 'orders_per_month', 'aov_trend', 'purchase_acceleration']\n",
    "    available_new_features = [f for f in new_features if f in df.columns]\n",
    "    if available_new_features:\n",
    "        print(\"New features created:\")\n",
    "        print(df[available_new_features].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b85bdd1a",
   "metadata": {},
   "source": [
    "# 5. Feature Selection\n",
    "Selection of the most relevant features for the model: checks which variables exist in the dataset, analyzes missing values, identifies and handles highly correlated columns to avoid multicollinearity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7961184b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Feature Selection\n",
    "if len(df) > 0:\n",
    "    # Select only features that definitely exist in our data\n",
    "    features = []\n",
    "    \n",
    "    # Add confirmed features\n",
    "    confirmed_features = [\n",
    "        'recency_days',\n",
    "        'total_orders', \n",
    "        'total_revenue',\n",
    "        'avg_order_value',\n",
    "        'customer_lifespan_days',\n",
    "        'recency_score',\n",
    "        'frequency_score', \n",
    "        'monetary_score',\n",
    "        'days_since_first_purchase',\n",
    "        'months_active'\n",
    "    ]\n",
    "    \n",
    "    # Add derived features if they exist\n",
    "    derived_features = [\n",
    "        'engagement_score',\n",
    "        'recency_log',\n",
    "        'orders_per_month',\n",
    "        'aov_trend',\n",
    "        'purchase_acceleration',\n",
    "        'customer_region'\n",
    "    ]\n",
    "    \n",
    "    # Build final feature list\n",
    "    for feature in confirmed_features + derived_features:\n",
    "        if feature in df.columns:\n",
    "            features.append(feature)\n",
    "    \n",
    "    print(f\"Selected {len(features)} features for modeling\")\n",
    "    print(\"Features:\", features)\n",
    "    \n",
    "    # Check for missing values\n",
    "    missing_values = df[features].isnull().sum()\n",
    "    if missing_values.any():\n",
    "        print(\"Missing values found:\")\n",
    "        print(missing_values[missing_values > 0])\n",
    "        \n",
    "        # Simple imputation\n",
    "        for col in features:\n",
    "            if df[col].isnull().any():\n",
    "                if df[col].dtype in ['int64', 'float64']:\n",
    "                    df[col] = df[col].fillna(df[col].median())\n",
    "                else:\n",
    "                    df[col] = df[col].fillna(df[col].mode()[0] if not df[col].mode().empty else 'Unknown')\n",
    "    \n",
    "    # Correlation analysis\n",
    "    numeric_cols = df[features].select_dtypes(include=[np.number]).columns\n",
    "    if len(numeric_cols) > 1:\n",
    "        correlation_matrix = df[numeric_cols].corr()\n",
    "        \n",
    "        # Identify highly correlated features (> 0.8)\n",
    "        high_corr = set()\n",
    "        for i in range(len(correlation_matrix.columns)):\n",
    "            for j in range(i):\n",
    "                if abs(correlation_matrix.iloc[i, j]) > 0.8:\n",
    "                    colname = correlation_matrix.columns[i]\n",
    "                    high_corr.add(colname)\n",
    "        \n",
    "        if high_corr:\n",
    "            print(f\"Highly correlated features to consider removing: {high_corr}\")\n",
    "            # Remove one from each highly correlated pair\n",
    "            for col in high_corr:\n",
    "                if col in features:\n",
    "                    features.remove(col)\n",
    "                    print(f\"Removed {col} due to high correlation\")\n",
    "    \n",
    "    print(f\"Final number of features after correlation check: {len(features)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae86d64",
   "metadata": {},
   "source": [
    "# 6. Data Preprocessing\n",
    "Data preparation for modeling: applies One-Hot Encoding for categorical variables, normalizes numerical features using StandardScaler, and ensures all data is in the correct format for algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912a03b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Data Preprocessing\n",
    "if len(df) > 0 and len(features) > 0:\n",
    "    # Create modeling dataframe\n",
    "    model_df = df[['customer_id', 'is_churned'] + features].copy()\n",
    "    \n",
    "    # Identify categorical variables\n",
    "    categorical_cols = []\n",
    "    for col in features:\n",
    "        if model_df[col].dtype == 'object' or model_df[col].nunique() < 10:\n",
    "            categorical_cols.append(col)\n",
    "    \n",
    "    # Separate numeric columns\n",
    "    numeric_cols = [col for col in features if col not in categorical_cols]\n",
    "    \n",
    "    print(f\"Categorical columns: {categorical_cols}\")\n",
    "    print(f\"Numeric columns: {len(numeric_cols)}\")\n",
    "    \n",
    "    # One-Hot Encoding for categorical variables\n",
    "    if categorical_cols:\n",
    "        model_df = pd.get_dummies(model_df, columns=categorical_cols, drop_first=True)\n",
    "    \n",
    "    # Update feature list after encoding\n",
    "    final_features = [col for col in model_df.columns if col not in ['customer_id', 'is_churned']]\n",
    "    \n",
    "    # Normalization\n",
    "    if numeric_cols:\n",
    "        # Only normalize columns that are still in the dataframe (not one-hot encoded)\n",
    "        numeric_to_normalize = [col for col in numeric_cols if col in model_df.columns]\n",
    "        if numeric_to_normalize:\n",
    "            scaler = StandardScaler()\n",
    "            model_df[numeric_to_normalize] = scaler.fit_transform(model_df[numeric_to_normalize])\n",
    "    \n",
    "    print(f\"Final dataset shape: {model_df.shape}\")\n",
    "    print(f\"Number of features: {len(final_features)}\")\n",
    "    print(f\"First 5 features: {final_features[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1d63f9",
   "metadata": {},
   "source": [
    "# 7. Train/Test Split\n",
    "Divides the dataset into training (80%) and test (20%) sets using stratification to maintain the same churn proportion in both sets. Validates whether the split preserved the original distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc0b76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Train/Test Split\n",
    "if len(model_df) > 0:\n",
    "    # Features and target\n",
    "    X = model_df[final_features]\n",
    "    y = model_df['is_churned']\n",
    "    \n",
    "    # Check if we have enough samples\n",
    "    if len(X) < 100:\n",
    "        print(\"Warning: Very small dataset size\")\n",
    "    \n",
    "    # Split 80/20 with stratification\n",
    "    try:\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, \n",
    "            test_size=0.2, \n",
    "            random_state=42,\n",
    "            stratify=y\n",
    "        )\n",
    "        \n",
    "        # Validate the split\n",
    "        print(f\"Training set: {len(X_train)} samples\")\n",
    "        print(f\"Test set: {len(X_test)} samples\")\n",
    "        print(f\"Churn rate in training: {y_train.mean():.2%}\")\n",
    "        print(f\"Churn rate in test: {y_test.mean():.2%}\")\n",
    "        \n",
    "        # Check if split preserved distribution\n",
    "        distribution_diff = abs(y_train.mean() - y_test.mean())\n",
    "        if distribution_diff > 0.05:\n",
    "            print(f\"Warning: Churn distribution not well preserved in split (diff: {distribution_diff:.3f})\")\n",
    "        else:\n",
    "            print(f\"Churn distribution well preserved (diff: {distribution_diff:.3f})\")\n",
    "            \n",
    "    except ValueError as e:\n",
    "        print(f\"Error in train_test_split: {e}\")\n",
    "        print(\"Using simple split without stratification...\")\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, \n",
    "            test_size=0.2, \n",
    "            random_state=42\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "037070dc",
   "metadata": {},
   "source": [
    "# 8. Model Training - Random Forest\n",
    "Trains a Random Forest Classifier model with parameters optimized to handle class imbalance (class_weight='balanced'). Evaluates performance on the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227e0432",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Model Training - Random Forest\n",
    "if len(X_train) > 0:\n",
    "    print(\"Training Random Forest model...\")\n",
    "    \n",
    "    # Calculate class weights for imbalance\n",
    "    class_weight = 'balanced'\n",
    "    \n",
    "    rf_model = RandomForestClassifier(\n",
    "        n_estimators=100,\n",
    "        max_depth=10,\n",
    "        min_samples_split=20,\n",
    "        min_samples_leaf=10,\n",
    "        random_state=42,\n",
    "        class_weight=class_weight,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    rf_model.fit(X_train, y_train)\n",
    "    \n",
    "    # Training predictions\n",
    "    y_train_pred = rf_model.predict(X_train)\n",
    "    y_train_proba = rf_model.predict_proba(X_train)[:, 1]\n",
    "    \n",
    "    print(\"Random Forest training complete\")\n",
    "    print(f\"Training Accuracy: {accuracy_score(y_train, y_train_pred):.3f}\")\n",
    "    print(f\"Training ROC-AUC: {roc_auc_score(y_train, y_train_proba):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e4df8c",
   "metadata": {},
   "source": [
    "# 9. Model Training - XGBoost\n",
    "Trains an XGBoost Classifier model configured with scale_pos_weight to balance imbalanced classes. Evaluates initial performance on the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22327727",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. Model Training - XGBoost\n",
    "if len(X_train) > 0:\n",
    "    print(\"Training XGBoost model...\")\n",
    "    \n",
    "    # Calculate scale_pos_weight for imbalance\n",
    "    scale_pos_weight = len(y_train[y_train == 0]) / len(y_train[y_train == 1])\n",
    "    \n",
    "    xgb_model = xgb.XGBClassifier(\n",
    "        n_estimators=100,\n",
    "        max_depth=6,\n",
    "        learning_rate=0.1,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        random_state=42,\n",
    "        scale_pos_weight=scale_pos_weight,\n",
    "        eval_metric='logloss',\n",
    "        use_label_encoder=False,\n",
    "        verbosity=0\n",
    "    )\n",
    "    \n",
    "    xgb_model.fit(X_train, y_train)\n",
    "    \n",
    "    # Training predictions\n",
    "    y_train_pred_xgb = xgb_model.predict(X_train)\n",
    "    y_train_proba_xgb = xgb_model.predict_proba(X_train)[:, 1]\n",
    "    \n",
    "    print(\"XGBoost training complete\")\n",
    "    print(f\"Training Accuracy: {accuracy_score(y_train, y_train_pred_xgb):.3f}\")\n",
    "    print(f\"Training ROC-AUC: {roc_auc_score(y_train, y_train_proba_xgb):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8458e8f",
   "metadata": {},
   "source": [
    "# 10. Model Comparison\n",
    "Compares both models on the test set using multiple metrics (Accuracy, ROC-AUC, Precision, Recall, F1-Score). Identifies which model has better overall performance based on ROC-AUC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c46add9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10. Model Comparison\n",
    "if len(X_test) > 0:\n",
    "    print(\"Model Comparison on Test Set\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Random Forest predictions\n",
    "    y_pred_rf = rf_model.predict(X_test)\n",
    "    y_proba_rf = rf_model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # XGBoost predictions\n",
    "    y_pred_xgb = xgb_model.predict(X_test)\n",
    "    y_proba_xgb = xgb_model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Compare metrics\n",
    "    models = ['Random Forest', 'XGBoost']\n",
    "    predictions = [(y_pred_rf, y_proba_rf), (y_pred_xgb, y_proba_xgb)]\n",
    "    \n",
    "    results = []\n",
    "    for i, (model_name, (y_pred, y_proba)) in enumerate(zip(models, predictions)):\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        roc_auc = roc_auc_score(y_test, y_proba)\n",
    "        \n",
    "        # Get classification report\n",
    "        report = classification_report(y_test, y_pred, output_dict=True)\n",
    "        precision = report['1']['precision'] if '1' in report else 0\n",
    "        recall = report['1']['recall'] if '1' in report else 0\n",
    "        f1 = report['1']['f1-score'] if '1' in report else 0\n",
    "        \n",
    "        results.append({\n",
    "            'Model': model_name,\n",
    "            'Accuracy': round(accuracy, 3),\n",
    "            'ROC-AUC': round(roc_auc, 3),\n",
    "            'Precision': round(precision, 3),\n",
    "            'Recall': round(recall, 3),\n",
    "            'F1-Score': round(f1, 3)\n",
    "        })\n",
    "    \n",
    "    # Create comparison dataframe\n",
    "    results_df = pd.DataFrame(results)\n",
    "    print(results_df.to_string(index=False))\n",
    "    \n",
    "    # Select best model based on ROC-AUC\n",
    "    best_model_idx = results_df['ROC-AUC'].idxmax()\n",
    "    best_model_name = results_df.loc[best_model_idx, 'Model']\n",
    "    best_model = rf_model if best_model_name == 'Random Forest' else xgb_model\n",
    "    \n",
    "    print(f\"\\nBest model: {best_model_name} with ROC-AUC: {results_df.loc[best_model_idx, 'ROC-AUC']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743359ce",
   "metadata": {},
   "source": [
    "# 11. Feature Importance Analysis\n",
    "Analyzes the most important features for the best model, displaying the top 15 variables that contribute most to churn predictions. Generates visualization for easy interpretation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4dd597",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11. Feature Importance Analysis\n",
    "print(\"Feature Importance Analysis\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Get feature importances from best model\n",
    "if 'best_model' in locals():\n",
    "    if hasattr(best_model, 'feature_importances_'):\n",
    "        importances = best_model.feature_importances_\n",
    "    elif hasattr(best_model, 'feature_importances'):\n",
    "        importances = best_model.feature_importances\n",
    "    \n",
    "    feature_importance_df = pd.DataFrame({\n",
    "        'feature': X_train.columns,\n",
    "        'importance': importances\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    # Display top 15 features\n",
    "    print(\"Top 15 Most Important Features:\")\n",
    "    print(feature_importance_df.head(15).to_string(index=False))\n",
    "    \n",
    "    # Visualization\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    top_features = feature_importance_df.head(15)\n",
    "    plt.barh(range(len(top_features)), top_features['importance'])\n",
    "    plt.yticks(range(len(top_features)), top_features['feature'])\n",
    "    plt.xlabel('Importance')\n",
    "    plt.title('Top 15 Feature Importances')\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add81459",
   "metadata": {},
   "source": [
    "# 12. Model Evaluation\n",
    "Complete evaluation of the best model: calculates ROC-AUC, generates classification report, confusion matrix, ROC curve, and Precision-Recall curve. Checks whether success criteria were met."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51385abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12. Model Evaluation\n",
    "if 'best_model' in locals() and len(X_test) > 0:\n",
    "    print(\"Model Evaluation\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Get predictions from best model\n",
    "    if best_model_name == 'Random Forest':\n",
    "        y_pred = y_pred_rf\n",
    "        y_proba = y_proba_rf\n",
    "    else:\n",
    "        y_pred = y_pred_xgb\n",
    "        y_proba = y_proba_xgb\n",
    "    \n",
    "    # A) ROC-AUC Score\n",
    "    roc_auc = roc_auc_score(y_test, y_proba)\n",
    "    print(f\"ROC-AUC Score: {roc_auc:.3f}\")\n",
    "    \n",
    "    # B) Classification Report\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    # C) Confusion Matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=['Not Churned', 'Churned'],\n",
    "                yticklabels=['Not Churned', 'Churned'])\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.show()\n",
    "    \n",
    "    # D) ROC Curve\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_proba)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(fpr, tpr, label=f'ROC Curve (AUC = {roc_auc:.3f})')\n",
    "    plt.plot([0, 1], [0, 1], 'k--', label='Random Classifier')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC Curve')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    # E) Precision-Recall Curve\n",
    "    precision_vals, recall_vals, _ = precision_recall_curve(y_test, y_proba)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(recall_vals, precision_vals)\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title('Precision-Recall Curve')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    # Check success criteria\n",
    "    print(\"\\nSuccess Criteria Check:\")\n",
    "    \n",
    "    report_dict = classification_report(y_test, y_pred, output_dict=True)\n",
    "    precision_score = report_dict['1']['precision'] if '1' in report_dict else 0\n",
    "    recall_score = report_dict['1']['recall'] if '1' in report_dict else 0\n",
    "    \n",
    "    # Use check marks instead of special characters\n",
    "    roc_check = \"PASS\" if roc_auc > 0.75 else \"FAIL\"\n",
    "    precision_check = \"PASS\" if precision_score > 0.60 else \"FAIL\"\n",
    "    recall_check = \"PASS\" if recall_score > 0.70 else \"FAIL\"\n",
    "    \n",
    "    print(f\"ROC-AUC > 0.75: {roc_check} ({roc_auc:.3f})\")\n",
    "    print(f\"Precision > 0.60: {precision_check} ({precision_score:.3f})\")\n",
    "    print(f\"Recall > 0.70: {recall_check} ({recall_score:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9268097d",
   "metadata": {},
   "source": [
    "# 13. Customer Risk Scoring\n",
    "Applies the trained model to all customers, assigning churn probability and classifying them into three risk levels (Low, Medium, High). Analyzes risk distribution by segments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eafa7535",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 13. Customer Risk Scoring\n",
    "if 'best_model' in locals() and len(model_df) > 0:\n",
    "    print(\"Customer Risk Scoring\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Score all customers\n",
    "    X_all = model_df[final_features]\n",
    "    model_df['churn_probability'] = best_model.predict_proba(X_all)[:, 1]\n",
    "    \n",
    "    # Classify by risk level\n",
    "    model_df['risk_level'] = pd.cut(\n",
    "        model_df['churn_probability'],\n",
    "        bins=[0, 0.3, 0.7, 1.0],\n",
    "        labels=['Low Risk', 'Medium Risk', 'High Risk'],\n",
    "        include_lowest=True\n",
    "    )\n",
    "    \n",
    "    # Get high risk customers\n",
    "    high_risk_customers = model_df[model_df['risk_level'] == 'High Risk'].sort_values(\n",
    "        'churn_probability', \n",
    "        ascending=False\n",
    "    )\n",
    "    \n",
    "    print(f\"Total customers: {len(model_df)}\")\n",
    "    print(f\"High risk customers: {len(high_risk_customers)} ({len(high_risk_customers)/len(model_df):.2%})\")\n",
    "    \n",
    "    # Display risk distribution\n",
    "    risk_counts = model_df['risk_level'].value_counts()\n",
    "    print(\"\\nRisk Distribution:\")\n",
    "    for level in ['Low Risk', 'Medium Risk', 'High Risk']:\n",
    "        count = risk_counts.get(level, 0)\n",
    "        print(f\"{level}: {count} ({count/len(model_df):.2%})\")\n",
    "    \n",
    "    # Analysis by segment if available\n",
    "    if 'rfm_segment' in df.columns and 'customer_id' in df.columns:\n",
    "        # Merge with original dataframe for segment analysis\n",
    "        temp_df = model_df[['customer_id', 'churn_probability']].merge(\n",
    "            df[['customer_id', 'rfm_segment']], \n",
    "            on='customer_id', \n",
    "            how='left'\n",
    "        )\n",
    "        risk_by_segment = temp_df.groupby('rfm_segment')['churn_probability'].mean().sort_values(ascending=False)\n",
    "        print(\"\\nTop 5 Segments by Churn Probability:\")\n",
    "        print(risk_by_segment.head(5))\n",
    "    \n",
    "    # Visualization\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    risk_counts.plot(kind='bar')\n",
    "    plt.title('Customer Risk Distribution')\n",
    "    plt.xlabel('Risk Level')\n",
    "    plt.ylabel('Number of Customers')\n",
    "    plt.xticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9167e041",
   "metadata": {},
   "source": [
    "# 14. Actionable Insights & Recommendations\n",
    "Generates actionable insights based on results: exports list of high-risk customers, provides recommendations for retention actions, presents success metrics, and suggests next steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b2efd2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 14. Actionable Insights & Recommendations\n",
    "print(\"Actionable Insights & Recommendations\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if 'high_risk_customers' in locals() and len(high_risk_customers) > 0:\n",
    "    # Export high risk customers\n",
    "    export_cols = ['customer_id', 'churn_probability', 'risk_level']\n",
    "    \n",
    "    # Add relevant features for analysis\n",
    "    feature_cols = ['recency_days', 'total_orders', 'avg_order_value', \n",
    "                   'engagement_score']\n",
    "    \n",
    "    # Only add columns that exist\n",
    "    for col in feature_cols:\n",
    "        if col in model_df.columns:\n",
    "            export_cols.append(col)\n",
    "    \n",
    "    high_risk_export = model_df.loc[high_risk_customers.index, export_cols].copy()\n",
    "    \n",
    "    # Merge with original dataframe for additional context if available\n",
    "    if 'customer_state' in df.columns and 'customer_id' in df.columns:\n",
    "        high_risk_export = high_risk_export.merge(\n",
    "            df[['customer_id', 'customer_state']], \n",
    "            on='customer_id', \n",
    "            how='left'\n",
    "        )\n",
    "    \n",
    "    if 'rfm_segment' in df.columns and 'customer_id' in df.columns:\n",
    "        high_risk_export = high_risk_export.merge(\n",
    "            df[['customer_id', 'rfm_segment']], \n",
    "            on='customer_id', \n",
    "            how='left'\n",
    "        )\n",
    "    \n",
    "    # Save to CSV\n",
    "    high_risk_export.to_csv('high_risk_customers.csv', index=False)\n",
    "    print(f\"Exported {len(high_risk_export)} high-risk customers to 'high_risk_customers.csv'\")\n",
    "    \n",
    "    # Save model results\n",
    "    if 'results_df' in locals():\n",
    "        results_df.to_csv('churn_model_results.csv', index=False)\n",
    "        print(\"Exported model results to 'churn_model_results.csv'\")\n",
    "    \n",
    "    # Save feature importance\n",
    "    if 'feature_importance_df' in locals():\n",
    "        feature_importance_df.to_csv('feature_importance.csv', index=False)\n",
    "        print(\"Exported feature importance to 'feature_importance.csv'\")\n",
    "    \n",
    "    # Key insights\n",
    "    print(\"\\nKey Insights:\")\n",
    "    print(\"1. Top factors driving churn:\")\n",
    "    if 'feature_importance_df' in locals():\n",
    "        top_factors = feature_importance_df.head(5)['feature'].tolist()\n",
    "        for i, factor in enumerate(top_factors, 1):\n",
    "            print(f\"   {i}. {factor}\")\n",
    "    \n",
    "    print(\"\\n2. Recommended Actions:\")\n",
    "    print(\"   - Implement retention campaigns for high-risk customers\")\n",
    "    print(\"   - Focus on improving recency (incentivize recent purchases)\")\n",
    "    print(\"   - Monitor engagement scores regularly\")\n",
    "    print(\"   - Create personalized offers based on customer segments\")\n",
    "    \n",
    "    print(\"\\n3. Success Metrics Achieved:\")\n",
    "    if 'roc_auc' in locals():\n",
    "        roc_auc_val = roc_auc\n",
    "        precision_val = precision_score if 'precision_score' in locals() else 0\n",
    "        recall_val = recall_score if 'recall_score' in locals() else 0\n",
    "        \n",
    "        print(f\"   - Model ROC-AUC: {roc_auc_val:.3f}\")\n",
    "        print(f\"   - Precision: {precision_val:.3f}\")\n",
    "        print(f\"   - Recall: {recall_val:.3f}\")\n",
    "        print(f\"   - High-risk customers identified: {len(high_risk_customers)}\")\n",
    "    \n",
    "    print(\"\\nNext Steps:\")\n",
    "    print(\"1. Deploy model for regular scoring (weekly/monthly)\")\n",
    "    print(\"2. Set up automated alerts for new high-risk customers\")\n",
    "    print(\"3. A/B test retention strategies\")\n",
    "    print(\"4. Monitor model performance over time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c70292",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up and final summary\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"CHURN PREDICTION PIPELINE COMPLETE\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "if 'best_model' in locals():\n",
    "    print(f\"Best Model: {best_model_name}\")\n",
    "    \n",
    "if 'roc_auc' in locals():\n",
    "    print(f\"Final Model Performance: ROC-AUC = {roc_auc:.3f}\")\n",
    "    \n",
    "if 'high_risk_customers' in locals():\n",
    "    print(f\"Customers at High Risk: {len(high_risk_customers)}\")\n",
    "\n",
    "print(\"\\nOutput Files Generated:\")\n",
    "print(\"1. high_risk_customers.csv - List of customers needing attention\")\n",
    "print(\"2. churn_model_results.csv - Model performance metrics\")\n",
    "print(\"3. feature_importance.csv - Key drivers of churn\")\n",
    "\n",
    "print(\"\\nPipeline execution complete\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
